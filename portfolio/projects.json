[
  {
    "slug": "trax-retail",
    "name": "Trax Retail - Shelf Recognition",
    "stack": ["Swift", "Objective-C", "C++", "OpenCV", "Pinecone", "Python", "Django", "Spark", "Firebase"],
    "period": "Jul 2022 - Aug 2023",
    "screenshots": ["trax-retail/application.png"],
    "description": "Leading CV company for retail. Built the flagship iOS app with on-device shelf recognition via OpenCV and designed a reusable internal SDK architecture for other Trax teams.",
    "highlights": [
      "OpenCV integration for real-time shelf image analysis on-device (Swift + C++)",
      "Designed internal SDK architecture — reusable CV pipeline for other Trax products",
      "Pinecone vector embeddings for product matching against catalog at scale",
      "Real-time mobile reporting for field reps",
      "Bridged CV/ML research models to production mobile code"
    ],
    "links": {
      "appStore": "https://apps.apple.com/ru/app/trax-retail/id1092946346"
    },
    "skillIds": ["swift-ios", "objective-c", "computer-vision-opencv", "python-django-fastapi", "rag-vector-search", "firebase", "system-architecture"],
    "writeup": "# Trax Retail - Shelf Recognition App\n\n**Period:** Jul 2022 – Aug 2023\n**Role:** Senior iOS Developer, SDK Architect\n\n## Overview\nTrax is a leading computer vision company for the retail industry. Their platform helps consumer goods manufacturers and retailers analyze shelf conditions in real-time — what's on the shelf, what's missing, what's misplaced. I joined the core mobile team to work on the flagship iOS app and build internal SDK infrastructure.\n\n## Stack\n- **Mobile:** Swift, Objective-C, C++\n- **CV/ML:** OpenCV (shelf image analysis), Pinecone (vector embeddings for product matching)\n- **Backend:** Python, Django\n- **Big Data:** Apache Spark\n- **Infrastructure:** Firebase, Git, Jira\n\n## Key Achievements\n- Engineered advanced computer vision integration via OpenCV for real-time shelf recognition — the app captures shelf images, processes them on-device, and sends structured data to the backend\n- Designed and built an internal SDK architecture for shelf recognition, packaging the CV pipeline as a reusable module for other Trax teams and products\n- Worked with Pinecone vector database for product matching — shelf images are embedded into vectors and matched against a product catalog for identification\n- Built real-time mobile reporting — field reps see shelf analysis results instantly on their devices\n- Collaborated closely with the CV/ML team to bridge the gap between research models and production mobile code\n\n## Technical Deep Dive\n**Challenge:** How do you run heavy computer vision (OpenCV) on a mobile device without killing performance, while keeping the SDK modular enough for other teams to integrate?\n\n**Solution:** Built a layered SDK architecture where the CV pipeline runs in a background processing queue. The SDK exposes a clean Swift API while the heavy lifting happens in C++/OpenCV under the hood. Image capture, preprocessing, and feature extraction are pipelined — while one frame is being analyzed, the next is already being captured. The Pinecone integration handles the matching step server-side, comparing extracted feature vectors against the product catalog at scale via Apache Spark batch processing.\n\n## Team & Culture\nWorked alongside talented engineers and managers (Daniel Stolero, Dolev Pomeranz, Alex Fishman, Youval Bronicki — all left LinkedIn recommendations). The team culture was collaborative and technically ambitious — pushing mobile CV capabilities forward together.\n\n## Links\n- [App Store](https://apps.apple.com/ru/app/trax-retail/id1092946346)"
  },
  {
    "slug": "scan-mania",
    "name": "Scan Mania - AR Shopping",
    "stack": ["Swift", "SwiftUI", "ARKit", "MLKit", "Unity3D", "FastAPI", "ASP.NET"],
    "period": "Oct 2020 - Oct 2022",
    "screenshots": ["scan-mania/game-screen.png", "scan-mania/challenges.png", "scan-mania/challenge-completed.png"],
    "description": "AR rewards app built from scratch with team of 3. Users scan barcodes on shelves using ARKit + MLKit, see 3D product overlays via Unity3D. Gamified shopping experience integrated with Shopkick.",
    "highlights": [
      "Built entire AR experience from scratch — ARKit + MLKit + Unity3D in one camera pipeline",
      "Real-time barcode detection while maintaining 3D spatial tracking",
      "Unity3D product visualizations overlaid on physical shelves",
      "FastAPI backend handling massive spatial data from AR sessions",
      "Gamified retail — scanning felt like collecting items in a game"
    ],
    "links": {
      "appStore": "https://apps.apple.com/il/app/scan-mania/id1540764114"
    },
    "skillIds": ["swift-ios", "swiftui", "arkit-scenekit", "unity3d-csharp", "aspnet-csharp", "firebase", "python-django-fastapi"],
    "writeup": "# Scan Mania - AR Shopping Application\n\n**Period:** Oct 2020 – Oct 2022\n**Role:** AR Developer, Backend Architect\n\n## Overview\nRewards app where users earn \"kicks\" by scanning barcodes on store shelves using Augmented Reality. Built from scratch with a team of 3 developers as an invite-only pilot integrated with Shopkick. Combined ARKit for spatial understanding, MLKit for barcode recognition, and Unity3D for 3D product visualizations — a genuinely fun shopping experience.\n\n## Stack\n- **Mobile:** Swift, SwiftUI, ARKit (augmented reality), MLKit (barcode recognition)\n- **3D:** Unity3D (product visualizations, 3D overlays on shelves)\n- **Backend:** FastAPI (Python), ASP.NET (C#)\n\n## Key Achievements\n- Built the entire AR experience from scratch — point your phone at a shelf and it recognizes products, highlights barcodes, and overlays 3D rewards\n- Integrated ARKit for spatial tracking + MLKit for real-time barcode detection — two ML frameworks working together in the same camera pipeline\n- Developed 3D product visualization overlays using Unity3D, leveraging prior game development experience (Cops Inc.)\n- Architected a FastAPI backend to handle massive spatial data volumes from AR sessions — each scan generates thousands of 3D coordinate points\n- Gamified the retail experience — scanning barcodes felt like collecting items in a game, driving high user engagement\n\n## Technical Challenges\n**Challenge:** Running ARKit (spatial tracking), MLKit (barcode detection), and Unity3D (3D rendering) simultaneously on a mobile device without frame drops.\n\n**Solution:** Separated the workloads across processing tiers. ARKit runs on the main camera feed for spatial anchoring. MLKit barcode detection runs on a secondary video buffer at reduced resolution. Unity3D renders only when ARKit confirms a stable surface — no wasted GPU cycles on unstable tracking. The backend handles the heavy aggregation — individual scan sessions push raw spatial data to FastAPI, which processes and matches against the product database asynchronously.\n\n## Links\n- [App Store](https://apps.apple.com/il/app/scan-mania/id1540764114)"
  },
  {
    "slug": "sos-portal",
    "name": "SOS Portal - COVID-19 Teleconsultation",
    "stack": ["ASP.NET", "React Native", "Binah", "REST APIs"],
    "period": "Mar 2020 - Oct 2020",
    "screenshots": ["sos-portal/main-screen.png", "sos-portal/incoming-call.png", "sos-portal/medical-card.png"],
    "description": "Teleconsultation platform for COVID-19 patient management in Brazil. Led team of 17 people.",
    "highlights": [
      "Video-based teleconsultations",
      "Deployed for Brazilian healthcare system",
      "Team of 17 (DevOps, mobile, web, backend, design, QA)",
      "Delivered in 6 months during pandemic"
    ],
    "links": {
      "playStore": "https://play.google.com/store/apps/details?id=com.ptm.sosportal"
    },
    "skillIds": ["aspnet-csharp", "react-native", "product-management", "webrtc", "system-architecture"],
    "writeup": "# SOS Portal - COVID-19 Teleconsultation Platform\n\n**Period:** Mar 2020 – Oct 2020\n**Role:** Team Lead (17 people)\n\n## Overview\nBuilt a teleconsultation platform for COVID-19 patient management, launched in Brazil. One of the most intense projects — delivered a complete healthcare platform in 6 months during the pandemic peak.\n\n## Team\nLed a team of 17 people:\n- 3 DevOps engineers\n- 2 React Native developers\n- 2 Web developers\n- 4 Backend engineers\n- 2 Designers\n- 1 Security specialist\n- 3 QA engineers\n\n## Stack\n- **Backend:** ASP.NET, REST APIs\n- **Mobile:** React Native\n- **Video:** Binah (video consultation platform)\n\n## Key Achievements\n- Launched for Brazilian healthcare system (still operational)\n- Enabled remote medical consultations during pandemic\n- Delivered in 6 months under extreme pressure\n- Strategic planning, team morale management, execution coordination\n\n## Impact\nDeployed for the Brazilian healthcare system, enabling thousands of remote consultations during the critical COVID-19 period. Platform is still operational today.\n\n## Leadership\n- Managed cross-functional team across DevOps, mobile, web, backend, design, security, and QA\n- Coordinated with stakeholders in high-pressure environment\n- Delivered mission-critical healthcare software on tight timeline\n\n## Links\n- [Google Play](https://play.google.com/store/apps/details?id=com.ptm.sosportal)"
  },
  {
    "slug": "bugsee",
    "name": "Bugsee - Developer Insights SDK",
    "stack": ["C", "C++", "Objective-C", "JavaScript", "CocoaPods", "SwiftPackages"],
    "period": "Jan 2016 - Jan 2020",
    "screenshots": ["bugsee/timeline.png", "bugsee/analytics.png"],
    "description": "Bug reporting SDK capturing video, network logs, and console output for mobile debugging.",
    "highlights": [
      "Automatic video capture of last minute",
      "Network traffic interceptor",
      "Custom memory zone for crash report invisibility",
      "Single line integration"
    ],
    "links": {
      "website": "https://www.bugsee.com"
    },
    "skillIds": ["objective-c", "swift-ios", "system-architecture"],
    "writeup": "# Bugsee - Developer Insights SDK\n\n**Period:** Jan 2016 – Jan 2020\n**Role:** Lead iOS Developer\n\n## Overview\nBug reporting SDK that captures video, network logs, and console output for mobile app debugging. Required deep iOS internals knowledge. Single line of init code unlocks full functionality.\n\n## Stack\n- **Core:** C, C++, Objective-C, JavaScript\n- **Distribution:** CocoaPods, SwiftPackages, Carthage\n- **Platforms:** iOS\n\n## Key Achievements\n- Created custom memory zone to stay invisible in crash reports (advanced iOS internals)\n- Built automatic video capture pipeline (last minute of screen recording)\n- Developed network traffic interceptor (captures all HTTP/HTTPS requests)\n- Implemented web view request logging\n- Added password field masking for security\n- Designed single-line integration API for developers\n\n## Technical Deep Dive\n**Challenge:** How do you capture everything happening in an app without interfering with crash reports or app behavior?\n\n**Solution:** Built a custom memory allocation zone that operates outside the standard iOS memory management, making Bugsee invisible to crash report systems while still capturing video, network traffic, and logs.\n\n## Team\nWorked with:\n- **Dmitry Fink** (now at Meta)\n- **Alex Fishman**\n\n## Domain Expertise\n- iOS internals (memory management, runtime)\n- Video capture and processing\n- Network traffic interception\n- SDK architecture and API design\n\n## Links\n- [bugsee.com](https://www.bugsee.com)"
  },
  {
    "slug": "minnow",
    "name": "Minnow - Streaming Aggregator",
    "stack": ["Swift", "iOS", "tvOS", "C#", "ASP.NET", "Firebase", "CocoaPods"],
    "period": "Dec 2019 - Feb 2020",
    "screenshots": ["minnow/search-ios.png", "minnow/main-page-tvos.png", "minnow/user-page-tvos.png"],
    "description": "Unified streaming aggregator combining Netflix, Disney+, Prime Video, and HBO. Alternative to Trakt.tv.",
    "highlights": [
      "Multi-platform (iPhone, iPad, Apple TV)",
      "Code-sharing between iOS and tvOS",
      "Backend development experience",
      "Cross-service discovery"
    ],
    "links": {
      "website": "https://minnowtv.com"
    },
    "skillIds": ["swift-ios", "aspnet-csharp", "firebase", "system-architecture"],
    "writeup": "# Minnow - Streaming Aggregator\n\n**Period:** Dec 2019 – Feb 2020\n**Role:** Lead iOS Developer + Backend\n\n## Overview\nUnified streaming aggregator combining libraries from Netflix, Disney+, Prime Video, and HBO into a single app. Alternative to Trakt.tv with discovery features across iOS and Apple TV.\n\n## Platforms\n- iPhone\n- iPad\n- Apple TV (tvOS)\n\n## Stack\n- **Mobile:** Swift, iOS, tvOS\n- **Backend:** C# (ASP.NET)\n- **Infrastructure:** Firebase, CocoaPods, REST APIs\n\n## Key Achievements\n- Code-sharing architecture between iOS and tvOS (divergent UI paradigms)\n- Built unified interface for multi-service streaming discovery\n- First major backend development experience (built MoviesDB-like database with Python dev)\n- Cross-platform development for iPhone, iPad, and Apple TV\n\n## Technical Challenge\n**Problem:** iOS and Apple TV have completely different interaction paradigms (touch vs remote), but need to share business logic and data management code.\n\n**Solution:** Architected shared Swift modules for networking, data models, and business logic, while maintaining platform-specific UI layers. Required careful abstraction design to avoid leaking platform-specific code into shared components.\n\n## Domain Expertise\n- Multi-platform iOS development\n- tvOS (Apple TV) development\n- Code sharing architecture\n- Backend development (ASP.NET + Python)\n- Media aggregation\n\n## Links\n- [minnowtv.com](https://minnowtv.com)"
  },
  {
    "slug": "thinkup",
    "name": "ThinkUp - Self-Improvement App",
    "stack": ["Swift", "Interface Builder", "iOS Frameworks", "Unit Testing"],
    "period": "Jan 2020 - Mar 2020",
    "screenshots": ["thinkup/dark-theme.png", "thinkup/light-theme.png", "thinkup/music.png"],
    "description": "Self-improvement app for building positive mindset through personalized affirmations.",
    "highlights": [
      "Dark/light theme support",
      "Published on App Store",
      "Audio affirmations",
      "User-friendly iOS UI"
    ],
    "links": {
      "appStore": "https://apps.apple.com/us/app/thinkup-positive-affirmations/id906660772"
    },
    "skillIds": ["swift-ios"],
    "writeup": "# ThinkUp - Self-Improvement App\n\n**Period:** Jan 2020 – Mar 2020\n**Role:** iOS Developer\n\n## Overview\nSelf-improvement app for building positive mindset through personalized affirmations. Published on App Store.\n\n## Stack\n- Swift\n- Interface Builder\n- iOS Frameworks\n- Unit Testing\n\n## Key Features\n- Dark/light theme support\n- Personalized affirmations\n- Audio playback for affirmations\n- Clean, polished iOS UI\n\n## Achievements\n- Published on App Store\n- Implemented dual-theme system (dark/light)\n- Built audio affirmation system\n- User-friendly interface focused on wellness\n\n## Domain\n- Consumer apps\n- Wellness / self-improvement\n- iOS UI/UX best practices\n\n## Links\n- [App Store](https://apps.apple.com/us/app/thinkup-positive-affirmations/id906660772)"
  },
  {
    "slug": "dishero",
    "name": "Dishero - Restaurant Discovery",
    "stack": ["Objective-C", "CoreData", "REST APIs"],
    "period": "May 2015 - May 2017",
    "screenshots": ["dishero/lineup.jpg"],
    "description": "San Francisco restaurant discovery app with dish-centric recommendations and complex filtering.",
    "highlights": [
      "Dish-centric discovery (not just restaurants)",
      "User-specific recommendations",
      "Complex data filters",
      "CoreData implementation"
    ],
    "links": {
      "appStore": "https://itunes.apple.com/us/app/dishero/id860283032"
    },
    "skillIds": ["objective-c"],
    "writeup": "# Dishero - Restaurant Discovery App\n\n**Period:** May 2015 – May 2017\n**Role:** Lead iOS Developer\n\n## Overview\nPilot project from San Francisco that aggregated restaurant menus, enabling dish discovery and user-centric recommendations. Focused on dishes, not just restaurants — a unique approach to food discovery.\n\n## Stack\n- Objective-C\n- CoreData\n- REST APIs\n- Redmine (project management)\n\n## Key Features\n- Dish-centric discovery (search by specific dishes, not just restaurants)\n- User-specific recommendations\n- Complex data filters for menu exploration\n- CoreData for local caching and performance\n\n## Technical Achievements\n- Built REST API integrations for aggregating menu data\n- Implemented complex filtering system for dish discovery\n- Developed recommendation engine for user preferences\n- CoreData architecture for offline functionality\n\n## Domain\n- Food tech\n- Discovery and recommendation systems\n- Data aggregation\n\n## Links\n- [App Store (archived)](https://itunes.apple.com/us/app/dishero/id860283032)"
  },
  {
    "slug": "performica",
    "name": "Performica - OrgGraph Network Analysis",
    "stack": ["Python", "Django", "Vue.js", "Scala", "WebSocket", "Figma"],
    "period": "Sep 2023 - Jun 2025",
    "screenshots": [],
    "description": "People analytics platform with organizational network visualization (OrgGraph), 360 reviews, and performance management. Product Engineer role: owned product vision, full-stack development, and design.",
    "highlights": [
      "Built OrgGraph — interactive network visualization of organizational relationships (hundreds/thousands of nodes)",
      "Rewrote entire product vision and user story flow from scratch",
      "360 review cycles, performance reviews, team health reporting",
      "Began LLM integration — caught REKAP's attention, led to acquisition",
      "Selected as 1 of 2 engineers (out of 7) to transition to REKAP"
    ],
    "links": {
      "website": "https://www.performica.com"
    },
    "skillIds": ["python-django-fastapi", "vue-js", "product-management", "llm-agents", "knowledge-graphs", "webrtc", "system-architecture"],
    "writeup": "# Performica - OrgGraph Network Analysis\n\n**Period:** Sep 2023 – Jun 2025\n**Role:** Product Engineer (officially Product Manager, but startup reality = product + engineering + design)\n\n## Overview\nPeople analytics and performance management platform — think Lattice, but with a focus on organizational network analysis. The core product, OrgGraph, visualized how people actually connect and collaborate across an organization — not just org charts, but real influence and communication patterns. The platform also included 360 review cycles, performance reviews, and team health reporting.\n\n## My Role\nAnother startup, another \"do everything\" situation. Owned the product vision for OrgGraph — rewrote the entire user story flow from scratch, defined the concept, and designed the experience end-to-end. On the engineering side, worked across the full stack on Django backend. Closely collaborated with my wife Anya on product design — we worked as a tight two-person unit, iterating fast on both the main platform and marketing website redesigns. Also participated in strategic product meetings, ran grooming sessions, and handled project management.\n\n## Stack\n- **Backend:** Django (Python), DRF\n- **Frontend:** Vue.js\n- **OrgGraph Engine:** Scala, WebSocket, optimized graph rendering library (high-performance visualization for large node counts)\n- **Design:** Figma (product + marketing redesigns)\n\n## Key Achievements\n- Designed and built OrgGraph — an interactive network visualization mapping real relationships and influence patterns across organizations, handling hundreds to thousands of nodes\n- Rewrote the entire product vision and user story flow for OrgGraph from the ground up\n- Built 360 review cycle workflows — multi-step feedback collection, calibration, and reporting (similar to Lattice's review process)\n- Redesigned both the main product UI and the marketing website\n- Began integrating LLM capabilities into the platform for automated insights — this AI direction caught REKAP's attention\n- Managed product strategy alongside engineering — strategic meetings, grooming sessions, roadmap planning\n\n## Technical Deep Dive\n**Challenge:** How do you render an interactive graph of hundreds (sometimes thousands) of organizational nodes with real-time relationship data without the browser choking?\n\n**Solution:** The OrgGraph engine ran on Scala with WebSocket connections pushing live data to the frontend. The visualization layer used a highly optimized graph rendering library built for large datasets — far beyond what D3.js can handle at scale. Nodes represented people, edges represented communication and collaboration patterns, and the system supported zooming, filtering, and clustering in real-time.\n\n## What Led to the Acquisition\nPerformica started integrating LLMs to generate automated insights from organizational data — performance summaries, relationship analysis, team health reports. REKAP saw the potential in how Performica collected, connected, and analyzed people data, and acquired the technology and team. Out of 7 engineers, only 2 (including me) were selected to transition to REKAP — bringing the domain knowledge and technical foundation that became part of REKAP's AI intelligence layer.\n\n## Design Partnership\nWorked side by side with Anya (my wife) on product design throughout the entire project. This turned out to be one of the most efficient design partnerships I've had — rapid iteration cycles, shared context, and zero communication overhead. Together we redesigned the product experience and the marketing website from the ground up.\n\n## Links\n- [performica.com](https://www.performica.com) (archived)\n- Similar product: [lattice.com](https://lattice.com)"
  },
  {
    "slug": "rekap",
    "name": "REKAP - AI Chief of Staff",
    "stack": ["React", "TypeScript", "Python", "Django", "LangChain", "LangGraph", "LiteLLM", "LiveKit", "PostgreSQL", "Redis", "Celery", "PostHog", "WebSocket"],
    "period": "Feb 2025 - Present",
    "screenshots": [],
    "description": "AI intelligence layer for organizations. Built real-time chat with WebSocket-driven agentic pipelines, voice agents, meeting scribe, and AI CRM. Startup role: product + full-stack + design.",
    "highlights": [
      "WebSocket chat orchestrating LangGraph agentic pipelines with streaming",
      "LiveKit voice agents for real-time meeting transcription",
      "Meeting scribe — automated action items, decisions, follow-ups",
      "SQL agent for natural language data queries",
      "Rolodex (AI CRM) — relationship mapping from meeting interactions",
      "Product design in Figma + PostHog analytics setup"
    ],
    "links": {
      "website": "https://www.rekap.com"
    },
    "skillIds": ["react-typescript", "python-django-fastapi", "langchain-langgraph", "llm-agents", "ai-agent-orchestration", "livekit", "postgresql", "redis-celery", "rag-vector-search", "whisper-openai", "product-management", "system-architecture"],
    "writeup": "# REKAP - AI Chief of Staff\n\n**Period:** Feb 2025 – Present\n**Role:** Product Engineer (officially Senior Full-Stack Engineer, but startup reality = product + full-stack + design)\n\n## Overview\nREKAP is an AI-powered intelligence layer for organizations — it captures meetings, models relationships, builds institutional memory, and uses AI agents to move work forward. Think of it as an AI chief of staff that listens, remembers, and acts. The platform serves leadership, talent, revenue, and operations teams with automated workflows, voice agents, and a smart CRM (Rolodex).\n\n## My Role\nStartup reality: wore every hat. Part product manager defining what to build, part full-stack engineer building it, part designer iterating in Figma. Configured PostHog analytics to track user behavior and inform product decisions. Worked in a team of 4 engineers.\n\n## Stack\n- **Frontend:** React, TypeScript, WebSocket (real-time chat)\n- **Backend:** Python, Django, DRF, Celery, Redis, PostgreSQL\n- **AI/ML:** LangChain, LangGraph (agentic pipelines), LiteLLM (multi-model routing)\n- **Voice:** LiveKit (real-time voice agents)\n- **Analytics:** PostHog\n- **Design:** Figma (product design iterations)\n\n## Key Achievements\n- Built a real-time chat system with WebSocket that orchestrates AI agent pipelines — user messages trigger LangGraph node chains that call tools, fetch context, and stream responses\n- Designed and implemented agentic pipelines with LangChain/LangGraph — multi-step chains where each node performs a specific task (retrieval, summarization, action extraction, etc.)\n- Developed LiveKit-powered voice agents that join meetings, transcribe in real-time, and generate structured summaries\n- Built the meeting scribe system — automated meeting intelligence that captures action items, decisions, and follow-ups\n- Created a SQL agent for natural language data queries — users ask questions in plain English, the agent generates and executes SQL\n- Implemented the Rolodex (AI CRM) — relationship mapping that builds contact records from meeting interactions automatically\n- Set up PostHog analytics pipeline to track engagement, feature adoption, and user flows\n- Iterated on product design in Figma — translating rough concepts into polished UI that shipped\n\n## Technical Deep Dive\n**Challenge:** How do you build a chat interface where each user message can trigger a complex, multi-step AI pipeline — with different nodes doing retrieval, reasoning, and tool calls — while keeping the UX responsive and streaming?\n\n**Solution:** Built a WebSocket-based chat layer on Django that routes incoming messages to LangGraph agentic chains. Each chain is a directed graph of nodes — one might fetch relevant meeting context via RAG, another runs the LLM with tool-calling enabled, another extracts action items. Results stream back to the client token-by-token via the WebSocket connection. Celery handles heavy async tasks (transcription, batch summarization) so the chat stays snappy.\n\n## What This Shows\n- Ability to operate across the full product lifecycle in a startup: ideation, design, implementation, analytics\n- Deep experience with modern AI/LLM tooling (LangChain, LangGraph, LiteLLM, LiveKit)\n- Real-time systems architecture (WebSocket, streaming, async task queues)\n- Product thinking — not just building features, but measuring their impact with PostHog and iterating on design in Figma\n\n## Links\n- [rekap.com](https://www.rekap.com)"
  },
  {
    "slug": "binaura",
    "name": "Binaura / SoundCalm - Sleep & Sound App",
    "stack": ["Swift", "SwiftUI", "StoreKit 2", "CloudKit", "Combine", "Firebase", "Next.js", "Python"],
    "period": "Sep 2025 - Present",
    "screenshots": [],
    "description": "Founder project: iOS app for sleep improvement and sound therapy. Multi-platform ecosystem with native iOS app (Live Activities, widgets), web app, Firebase backend, and a Python analytics pipeline processing ~8k data points.",
    "highlights": [
      "Real revenue: ~$50k/year, organic growth",
      "Subscription model via Adapty, App Store featured multiple times",
      "iOS Live Activities + Home Screen widgets",
      "Web app (Next.js) with embedding server",
      "Analytics pipeline (Python) for business intelligence",
      "Team of 3, full product ownership"
    ],
    "links": {},
    "skillIds": ["swift-ios", "swiftui", "react-typescript", "python-django-fastapi", "firebase", "product-management"],
    "writeup": "# Binaura / SoundCalm - Sleep & Sound App\n\n**Period:** Sep 2025 – Present\n**Role:** Founder, Full-Stack Developer\n\n## Overview\niOS app for sleep improvement and sound therapy. Real revenue-generating independent project with organic growth and App Store featuring. Multi-platform ecosystem with native iOS app (Live Activities, widgets), web app, Firebase backend, and a Python analytics pipeline.\n\n## Business Metrics\n- **Revenue:** ~$50k/year\n- **Growth:** Organic (no paid acquisition)\n- **Recognition:** App Store featured multiple times\n- **Team:** 3 people (developer, designer, backend)\n\n## Stack\n- **iOS:** Swift, SwiftUI, StoreKit 2, CloudKit, Combine\n- **Backend:** Firebase\n- **Web:** Next.js\n- **Analytics:** Python (pipeline processing ~8k data points)\n- **Features:** iOS Live Activities, Home Screen widgets\n\n## Key Achievements\n- Built and launched revenue-generating iOS app\n- Subscription model via Adapty (payment management)\n- App Store featured multiple times\n- Organic user acquisition and growth\n- Product decisions, development, and App Store management\n\n## Business Skills\n- App Store optimization\n- Subscription business model\n- Product strategy\n- User acquisition\n- Team coordination (designer, backend developer)\n\n## Technical Skills\n- Modern SwiftUI development\n- StoreKit 2 integration\n- CloudKit sync\n- Combine reactive patterns\n- Audio processing and playback\n\n## What This Shows\nThis project demonstrates:\n1. **Product thinking** — identified market need, built solution\n2. **Full ownership** — end-to-end development and business management\n3. **Revenue generation** — not just a side project, a real business\n4. **Sustained effort** — active development and continuous improvement\n5. **Team leadership** — coordinated designer and backend developer"
  },
  {
    "slug": "cops-inc",
    "name": "Cops Inc. - GTA-Style Mobile Action Game",
    "stack": ["Unity3D", "C#", "Dijkstra", "Octree"],
    "period": "Jun 2021 - Jun 2024",
    "screenshots": ["cops-inc/alpha-test.jpg"],
    "description": "Co-founded indie game studio. Open-world mobile game with GTA-style gameplay, traffic simulation, drivable vehicles, and mission-based routing powered by Dijkstra and Octree algorithms.",
    "highlights": [
      "Game studio co-founder, built from scratch with a friend",
      "Dijkstra pathfinding for mission routing across open-world map",
      "Octree spatial partitioning for optimized entity queries",
      "Traffic AI simulation with road-network navigation",
      "Collaborated with ex-Rovio (Angry Birds) designers"
    ],
    "links": {},
    "skillIds": ["unity3d-csharp"],
    "writeup": "# Cops Inc. - GTA-Style Mobile Action Game\n\n**Period:** Jun 2021 – Jun 2024\n**Role:** Co-Founder & Developer\n\n## Overview\nOpen-world mobile game where you play as a cop in a GTA-inspired city. Features a full traffic simulation system, drivable vehicles, mission-based gameplay, and pathfinding across a large open map. Built as a passion project with a friend — equal parts self-education and indie game ambition.\n\n## Stack\n- **Engine:** Unity3D, C#\n- **Algorithms:** Dijkstra (pathfinding), Octree (spatial partitioning)\n- **Design:** Collaborated with ex-Rovio (Angry Birds) designers\n\n## Key Achievements\n- Built a traffic simulation system with AI-driven vehicles following road networks\n- Implemented Dijkstra-based pathfinding for mission routing across the open world\n- Developed Octree spatial partitioning for optimized collision detection and entity queries\n- Created a vehicle system with multiple drivable transport types\n- Designed mission framework with dynamic objective routing\n\n## Technical Deep Dive\n**Challenge:** How do you route a player (or AI) to mission objectives across a large open-world map without killing performance on mobile?\n\n**Solution:** Combined Dijkstra's algorithm for graph-based road network pathfinding with an Octree spatial structure for fast entity lookups and proximity queries. The Octree dramatically reduced the search space for collision detection and nearby-entity checks, while Dijkstra handled optimal route calculation through the city's road graph. This kept the game running smoothly even with dozens of traffic AI agents simultaneously navigating the map.\n\n## What We Learned\nThis was a self-education project at its core. Diving into classical CS algorithms (Dijkstra, Octree) in a real-world game context was far more instructive than any textbook. We ran one paid traffic test through friends at Daki (a UA company) — the retention metrics weren't strong enough to justify scaling, and neither of us had the bandwidth to iterate further. The project stayed in the drawer, but the engineering experience stuck.\n\n## Team\n- **Andrey Kovalev** — development, game systems\n- **Co-founder** — game design, level design\n- **Ex-Rovio designers** — art direction, UI/UX consulting"
  },
  {
    "slug": "ecoiq",
    "name": "EcoIQ - ESG Supply Chain Platform",
    "stack": ["Next.js", "FastAPI", "Python", "LLM"],
    "period": "2025 - Present",
    "screenshots": [],
    "description": "ESG reporting and supply chain risk management platform. Architecture consultant role: migrating from monolithic Next.js to FastAPI + Next.js, integrating LLM capabilities for automated ESG insights.",
    "highlights": [
      "Architecture consulting — redesigning system from monolithic Next.js to FastAPI + Next.js",
      "LLM integration for automated ESG analysis and reporting",
      "Mentoring the engineering team on backend architecture best practices",
      "Supply chain ESG compliance (CSRD, ESRS, ISSB standards)"
    ],
    "links": {
      "website": "https://eco-iq.com"
    },
    "skillIds": ["react-typescript", "python-django-fastapi", "llm-agents", "system-architecture"],
    "writeup": "# EcoIQ - ESG Supply Chain Platform\n\n**Period:** 2025 – Present\n**Role:** Architecture Consultant & LLM Expert\n\n## Overview\nESG reporting and supply chain risk management platform based in Amsterdam. EcoIQ helps companies create transparency across their supplier ecosystems — identifying ESG risks (human rights violations, environmental hazards, energy risks) and ensuring regulatory compliance (CSRD, ESRS, ISSB). Think of it as automated supply chain due diligence powered by AI.\n\n## My Role\nPart-time consulting engagement alongside REKAP (which remains the primary role). Brought in as an architecture expert to fix foundational technical issues left by previous developers and integrate LLM capabilities.\n\n## Stack\n- **Current:** Next.js (monolithic — frontend + backend combined)\n- **Migrating to:** FastAPI (Python) backend + Next.js frontend (proper separation)\n- **AI:** LLM integration for automated ESG analysis and insights\n\n## Key Achievements\n- Designed the migration architecture: splitting monolithic Next.js into a proper FastAPI backend + Next.js frontend\n- Integrating LLM capabilities for automated ESG data analysis and report generation\n- Mentoring the engineering team on backend architecture, API design, and best practices\n- Helping the team understand and implement proper separation of concerns\n\n## Technical Challenge\n**Problem:** The previous developers built everything inside Next.js — API routes handling heavy business logic, no proper backend service layer, unoptimized data flows. This doesn't scale for an ESG platform that needs to process supplier data, run compliance checks, and generate reports.\n\n**Solution:** Architecting a clean FastAPI backend with proper service/repository layers, async task processing, and LLM integration points. The Next.js frontend becomes a thin client consuming a well-defined API. This unlocks the ability to add LLM-powered features (automated ESG scoring, natural language report generation) without fighting the framework.\n\n## Links\n- [eco-iq.com](https://eco-iq.com)"
  }
]
